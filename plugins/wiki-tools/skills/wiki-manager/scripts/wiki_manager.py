#!/usr/bin/env python3
"""
Confluence Wiki Manager - å®Œæ•´çš„ Wiki é¡µé¢ç®¡ç†å·¥å…·

åŠŸèƒ½ï¼š
- è·å– Wiki é¡µé¢å†…å®¹ï¼ˆæ”¯æŒ Markdown/HTML æ ¼å¼ï¼‰
- æ›´æ–° Wiki é¡µé¢å†…å®¹å’Œæ ‡é¢˜
- è¿½åŠ å†…å®¹åˆ°ç°æœ‰é¡µé¢
- åˆ›å»ºæ–°é¡µé¢ï¼ˆæ”¯æŒå¤§å†…å®¹è‡ªåŠ¨åˆ†æ‰¹ï¼‰
- ä» URL è‡ªåŠ¨æå–é¡µé¢ ID

ç¯å¢ƒå˜é‡é…ç½®ï¼š
- WIKI_BASE_URL: Confluence åŸºç¡€ URLï¼ˆé»˜è®¤: https://wiki.*.comï¼‰
- WIKI_TOKEN: Confluence API Tokenï¼ˆå¿…éœ€ï¼‰
- WIKI_DEFAULT_SPACE: é»˜è®¤ç©ºé—´ keyï¼ˆå¯é€‰ï¼Œä¾‹å¦‚: ~htï¼‰
- WIKI_DEFAULT_PARENT_PAGE: é»˜è®¤çˆ¶é¡µé¢ IDï¼ˆå¯é€‰ï¼Œä¾‹å¦‚: 217851921ï¼‰
"""

import os
import re
import sys
import json
import asyncio
import argparse
from typing import Optional
from markdownify import markdownify as md

try:
    import httpx
except ImportError:
    print("é”™è¯¯ï¼šéœ€è¦å®‰è£… httpx åº“")
    print("è¿è¡Œ: pip install httpx")
    sys.exit(1)

try:
    import markdown
except ImportError:
    markdown = None


# ============================================================================
# é…ç½®ç®¡ç†
# ============================================================================

class WikiConfig:
    """Wiki é…ç½®ç®¡ç†"""

    def __init__(self):
        self.base_url = os.getenv("WIKI_BASE_URL", "https://wiki.*.com")
        self.token = os.getenv("WIKI_TOKEN", "")
        self.default_space = os.getenv("WIKI_DEFAULT_SPACE", "")
        self.default_parent_page_id = os.getenv("WIKI_DEFAULT_PARENT_PAGE", "")

        if not self.token.strip():
            raise ValueError(
                "æœªé…ç½® WIKI_TOKEN ç¯å¢ƒå˜é‡\n"
                "è¯·è®¾ç½®: export WIKI_TOKEN='your-token-here'"
            )

    def get_auth_headers(self) -> dict:
        """ç”Ÿæˆè®¤è¯è¯·æ±‚å¤´"""
        return {
            "Authorization": f"Bearer {self.token}",
            "Content-Type": "application/json",
            "Accept": "application/json"
        }


# ============================================================================
# HTTP å®¢æˆ·ç«¯
# ============================================================================

async def fetch_json(
    url: str,
    headers: dict,
    params: Optional[dict] = None,
    timeout: float = 30.0
) -> dict:
    """é€šç”¨ HTTP GET è¯·æ±‚"""
    async with httpx.AsyncClient() as client:
        try:
            response = await client.get(
                url,
                headers=headers,
                params=params,
                timeout=timeout
            )
            response.raise_for_status()
            return {"success": True, "data": response.json()}

        except httpx.HTTPStatusError as e:
            status = e.response.status_code
            error_map = {
                401: "è®¤è¯å¤±è´¥ï¼Œè¯·æ£€æŸ¥ Token",
                403: "æƒé™ä¸è¶³",
                404: "èµ„æºä¸å­˜åœ¨",
                429: "è¯·æ±‚è¿‡äºé¢‘ç¹ï¼Œè¯·ç¨åé‡è¯•"
            }
            return {
                "success": False,
                "error": error_map.get(status, f"HTTP {status} é”™è¯¯"),
                "status_code": status
            }

        except Exception as e:
            return {"success": False, "error": str(e)}


async def put_json(
    url: str,
    headers: dict,
    data: dict,
    timeout: float = 30.0
) -> dict:
    """é€šç”¨ HTTP PUT è¯·æ±‚"""
    async with httpx.AsyncClient() as client:
        try:
            response = await client.put(
                url,
                headers=headers,
                json=data,
                timeout=timeout
            )
            response.raise_for_status()
            return {"success": True, "data": response.json()}

        except httpx.HTTPStatusError as e:
            status = e.response.status_code
            error_map = {
                401: "è®¤è¯å¤±è´¥ï¼Œè¯·æ£€æŸ¥ Token",
                403: "æƒé™ä¸è¶³ï¼Œè¯·æ£€æŸ¥æ˜¯å¦æœ‰ç¼–è¾‘æƒé™",
                404: "èµ„æºä¸å­˜åœ¨",
                409: "ç‰ˆæœ¬å†²çªï¼Œé¡µé¢å·²è¢«å…¶ä»–äººä¿®æ”¹",
                429: "è¯·æ±‚è¿‡äºé¢‘ç¹ï¼Œè¯·ç¨åé‡è¯•"
            }
            return {
                "success": False,
                "error": error_map.get(status, f"HTTP {status} é”™è¯¯"),
                "status_code": status
            }

        except Exception as e:
            return {"success": False, "error": str(e)}


async def post_json(
    url: str,
    headers: dict,
    data: dict,
    timeout: float = 30.0
) -> dict:
    """é€šç”¨ HTTP POST è¯·æ±‚"""
    async with httpx.AsyncClient() as client:
        try:
            response = await client.post(
                url,
                headers=headers,
                json=data,
                timeout=timeout
            )
            response.raise_for_status()
            return {"success": True, "data": response.json()}

        except httpx.HTTPStatusError as e:
            status = e.response.status_code
            error_map = {
                401: "è®¤è¯å¤±è´¥ï¼Œè¯·æ£€æŸ¥ Token",
                403: "æƒé™ä¸è¶³ï¼Œè¯·æ£€æŸ¥æ˜¯å¦æœ‰åˆ›å»ºé¡µé¢æƒé™",
                404: "èµ„æºä¸å­˜åœ¨",
                400: "è¯·æ±‚å‚æ•°é”™è¯¯",
                429: "è¯·æ±‚è¿‡äºé¢‘ç¹ï¼Œè¯·ç¨åé‡è¯•"
            }
            return {
                "success": False,
                "error": error_map.get(status, f"HTTP {status} é”™è¯¯"),
                "status_code": status
            }

        except Exception as e:
            return {"success": False, "error": str(e)}


# ============================================================================
# æ ¸å¿ƒåŠŸèƒ½
# ============================================================================

def extract_page_id(page_url: str) -> str:
    """ä» URL æå–é¡µé¢ ID

    æ”¯æŒçš„ URL æ ¼å¼ï¼š
    - https://wiki.*.com/pages/12345678
    - https://wiki.*.com/pages/viewpage.action?pageId=12345678
    """
    # æ ¼å¼1: /pages/12345678
    match = re.search(r'/pages/(\d+)', page_url)
    if match:
        return match.group(1)

    # æ ¼å¼2: pageId=12345678
    match = re.search(r'pageId=(\d+)', page_url)
    if match:
        return match.group(1)

    raise ValueError(f"æ— æ³•ä» URL æå–é¡µé¢ ID: {page_url}")


async def get_wiki_page_content(
    config: WikiConfig,
    page_id: Optional[str] = None,
    page_url: Optional[str] = None,
    format: str = "markdown"
) -> dict:
    """è·å– Wiki é¡µé¢å†…å®¹

    Args:
        config: Wiki é…ç½®
        page_id: é¡µé¢ ID
        page_url: é¡µé¢ URLï¼ˆå¦‚æœæä¾›åˆ™è‡ªåŠ¨æå– page_idï¼‰
        format: è¾“å‡ºæ ¼å¼ï¼Œ'markdown'ï¼ˆé»˜è®¤ï¼‰ã€'storage'ï¼ˆHTMLï¼‰æˆ– 'view'

    Returns:
        åŒ…å«é¡µé¢ä¿¡æ¯çš„å­—å…¸
    """
    # å¤„ç† page_url
    if page_url and not page_id:
        page_id = extract_page_id(page_url)

    if not page_id:
        raise ValueError("å¿…é¡»æä¾› page_id æˆ– page_url")

    url = f"{config.base_url}/rest/api/content/{page_id}"
    headers = config.get_auth_headers()
    params = {
        "expand": "body.storage,body.view,version,space,metadata.labels,children.attachment"
    }

    result = await fetch_json(url, headers, params)

    if not result["success"]:
        raise RuntimeError(result["error"])

    # è§£ææ•°æ®
    data = result["data"]

    # é€‰æ‹©å†…å®¹æ ¼å¼
    if format == "storage":
        content = data.get("body", {}).get("storage", {}).get("value", "")
    elif format == "view":
        content = data.get("body", {}).get("view", {}).get("value", "")
    else:  # markdown
        html_content = data.get("body", {}).get("storage", {}).get("value", "")
        content = md(html_content, heading_style="ATX")

    parsed = {
        "id": data["id"],
        "title": data["title"],
        "url": f"{config.base_url}/pages/viewpage.action?pageId={data['id']}",
        "space": data.get("space", {}).get("key", ""),
        "content": content,
        "version": data.get("version", {}).get("number", 0),
        "last_updated": data.get("version", {}).get("when", ""),
        "last_updated_by": data.get("version", {}).get("by", {}).get("displayName", ""),
        "labels": [
            label["name"]
            for label in data.get("metadata", {}).get("labels", {}).get("results", [])
        ]
    }

    # é™„ä»¶
    attachments = data.get("children", {}).get("attachment", {}).get("results", [])
    parsed["attachments"] = [
        {
            "filename": a.get("title", ""),
            "size": a.get("extensions", {}).get("fileSize", 0),
            "url": f"{config.base_url}{a.get('_links', {}).get('download', '')}"
        }
        for a in attachments
    ]

    return parsed


async def update_wiki_page_content(
    config: WikiConfig,
    page_id: str,
    content: Optional[str] = None,
    title: Optional[str] = None,
    format: str = "markdown",
    append: bool = False
) -> dict:
    """æ›´æ–° Wiki é¡µé¢å†…å®¹

    Args:
        config: Wiki é…ç½®
        page_id: é¡µé¢ ID
        content: æ–°å†…å®¹ï¼ˆå¦‚æœä¸ºç©ºåˆ™ä¸ä¿®æ”¹å†…å®¹ï¼‰
        title: æ–°æ ‡é¢˜ï¼ˆå¦‚æœä¸ºç©ºåˆ™ä¸ä¿®æ”¹æ ‡é¢˜ï¼‰
        format: å†…å®¹æ ¼å¼ï¼Œ'markdown'ï¼ˆé»˜è®¤ï¼‰æˆ– 'html'
        append: æ˜¯å¦è¿½åŠ å†…å®¹ï¼ˆTrue=è¿½åŠ åˆ°æœ«å°¾ï¼ŒFalse=è¦†ç›–ï¼‰

    Returns:
        æ›´æ–°åçš„é¡µé¢ä¿¡æ¯
    """
    if not content and not title:
        raise ValueError("è‡³å°‘éœ€è¦æä¾› content æˆ– title")

    # 1. è·å–å½“å‰é¡µé¢ä¿¡æ¯ï¼ˆéœ€è¦ç‰ˆæœ¬å·ï¼‰
    current_page = await get_wiki_page_content(config, page_id=page_id, format="storage")
    current_version = current_page["version"]
    current_title = current_page["title"]
    current_content_html = current_page["content"]

    # 2. å¤„ç†æ–°å†…å®¹
    if content:
        # è½¬æ¢ Markdown ä¸º HTMLï¼ˆå¦‚æœéœ€è¦ï¼‰
        if format == "markdown":
            if markdown is None:
                raise RuntimeError("éœ€è¦å®‰è£… markdown åº“: pip install markdown")
            new_content_html = markdown.markdown(content, extensions=['extra', 'nl2br'])
        else:  # html
            new_content_html = content

        # è¿½åŠ æ¨¡å¼ï¼šåœ¨åŸæœ‰å†…å®¹åæ·»åŠ 
        if append:
            final_content_html = current_content_html + "\n" + new_content_html
        else:
            final_content_html = new_content_html
    else:
        # ä¸ä¿®æ”¹å†…å®¹
        final_content_html = current_content_html

    # 3. å¤„ç†æ ‡é¢˜
    final_title = title if title else current_title

    # 4. æ„é€ æ›´æ–°è¯·æ±‚
    url = f"{config.base_url}/rest/api/content/{page_id}"
    headers = config.get_auth_headers()

    update_data = {
        "version": {"number": current_version + 1},
        "title": final_title,
        "type": "page",
        "body": {
            "storage": {
                "value": final_content_html,
                "representation": "storage"
            }
        }
    }

    # 5. å‘é€ PUT è¯·æ±‚
    result = await put_json(url, headers, update_data)

    if not result["success"]:
        raise RuntimeError(f"æ›´æ–° Wiki é¡µé¢å¤±è´¥: {result['error']}")

    # 6. è§£æè¿”å›æ•°æ®
    data = result["data"]
    return {
        "id": data["id"],
        "title": data["title"],
        "url": f"{config.base_url}/pages/viewpage.action?pageId={data['id']}",
        "version": data.get("version", {}).get("number", 0),
        "last_updated": data.get("version", {}).get("when", ""),
        "last_updated_by": data.get("version", {}).get("by", {}).get("displayName", ""),
        "message": f"é¡µé¢å·²æˆåŠŸæ›´æ–°åˆ°ç‰ˆæœ¬ {data.get('version', {}).get('number', 0)}"
    }


async def create_wiki_page(
    config: WikiConfig,
    title: str,
    content: str,
    space_key: str,
    format: str = "html",
    parent_page_id: Optional[str] = None
) -> dict:
    """åˆ›å»ºæ–°çš„ Wiki é¡µé¢

    Args:
        config: Wiki é…ç½®
        title: é¡µé¢æ ‡é¢˜ï¼ˆå¿…éœ€ï¼‰
        content: é¡µé¢å†…å®¹ï¼ˆå¿…éœ€ï¼‰
        space_key: ç©ºé—´ keyï¼ˆä¾‹å¦‚: "~ht", "SPACE" ç­‰ï¼‰
        format: å†…å®¹æ ¼å¼ï¼Œ'html'ï¼ˆé»˜è®¤ï¼Œæ¨èï¼‰æˆ– 'markdown'
        parent_page_id: çˆ¶é¡µé¢ IDï¼ˆå¦‚æœä¸ºç©ºåˆ™åˆ›å»ºé¡¶çº§é¡µé¢ï¼‰

    Returns:
        æ–°åˆ›å»ºçš„é¡µé¢ä¿¡æ¯
    """
    if not title:
        raise ValueError("å¿…é¡»æä¾›é¡µé¢æ ‡é¢˜")

    if not content:
        raise ValueError("å¿…é¡»æä¾›é¡µé¢å†…å®¹")

    if not space_key:
        raise ValueError("å¿…é¡»æä¾›ç©ºé—´ key")

    # 1. å¤„ç†å†…å®¹æ ¼å¼
    if format == "markdown":
        if markdown is None:
            raise RuntimeError("éœ€è¦å®‰è£… markdown åº“: pip install markdown")
        content_html = markdown.markdown(content, extensions=['extra', 'nl2br'])
    else:  # html
        content_html = content

    # 2. æ„é€ åˆ›å»ºé¡µé¢è¯·æ±‚
    url = f"{config.base_url}/rest/api/content"
    headers = config.get_auth_headers()

    create_data = {
        "type": "page",
        "title": title,
        "space": {"key": space_key},
        "status": "current",
        "body": {
            "storage": {
                "value": content_html,
                "representation": "storage"
            }
        }
    }

    # 3. å¦‚æœæŒ‡å®šäº†çˆ¶é¡µé¢ï¼Œæ·»åŠ åˆ° ancestors
    if parent_page_id:
        create_data["ancestors"] = [{"id": parent_page_id}]

    # 4. å‘é€ POST è¯·æ±‚
    result = await post_json(url, headers, create_data)

    if not result["success"]:
        raise RuntimeError(f"åˆ›å»º Wiki é¡µé¢å¤±è´¥: {result['error']}")

    # 5. è§£æè¿”å›æ•°æ®
    data = result["data"]
    return {
        "id": data["id"],
        "title": data["title"],
        "url": f"{config.base_url}/pages/viewpage.action?pageId={data['id']}",
        "space": data.get("space", {}).get("key", ""),
        "version": data.get("version", {}).get("number", 0),
        "created": data.get("version", {}).get("when", ""),
        "created_by": data.get("version", {}).get("by", {}).get("displayName", ""),
        "message": f"é¡µé¢å·²æˆåŠŸåˆ›å»ºï¼ŒID: {data['id']}"
    }


async def create_wiki_page_with_chunks(
    config: WikiConfig,
    title: str,
    content: str,
    space_key: str,
    format: str = "html",
    parent_page_id: Optional[str] = None,
    chunk_size: int = 1024 * 1024  # é»˜è®¤ 1MB
) -> dict:
    """åˆ›å»º Wiki é¡µé¢ï¼Œå¦‚æœå†…å®¹è¿‡é•¿åˆ™åˆ†æ‰¹è¿½åŠ 

    Args:
        config: Wiki é…ç½®
        title: é¡µé¢æ ‡é¢˜ï¼ˆå¿…éœ€ï¼‰
        content: é¡µé¢å†…å®¹ï¼ˆå¿…éœ€ï¼‰
        space_key: ç©ºé—´ keyï¼ˆä¾‹å¦‚: "~ht", "SPACE" ç­‰ï¼‰
        format: å†…å®¹æ ¼å¼ï¼Œ'html'ï¼ˆé»˜è®¤ï¼Œæ¨èï¼‰æˆ– 'markdown'
        parent_page_id: çˆ¶é¡µé¢ IDï¼ˆå¦‚æœä¸ºç©ºåˆ™åˆ›å»ºé¡¶çº§é¡µé¢ï¼‰
        chunk_size: æ¯æ‰¹å†…å®¹çš„æœ€å¤§å­—èŠ‚æ•°ï¼ˆé»˜è®¤: 1MBï¼‰

    Returns:
        åˆ›å»ºçš„é¡µé¢ä¿¡æ¯ï¼ŒåŒ…å«åˆ†æ‰¹ç»Ÿè®¡
    """
    if not title:
        raise ValueError("å¿…é¡»æä¾›é¡µé¢æ ‡é¢˜")

    if not content:
        raise ValueError("å¿…é¡»æä¾›é¡µé¢å†…å®¹")

    if not space_key:
        raise ValueError("å¿…é¡»æä¾›ç©ºé—´ key")

    # 1. å¤„ç†å†…å®¹æ ¼å¼
    if format == "markdown":
        if markdown is None:
            raise RuntimeError("éœ€è¦å®‰è£… markdown åº“: pip install markdown")
        content_html = markdown.markdown(content, extensions=['extra', 'nl2br'])
    else:  # html
        content_html = content

    # 2. è®¡ç®—å†…å®¹å¤§å°
    content_bytes = content_html.encode('utf-8')
    total_size = len(content_bytes)

    # 3. å¦‚æœå†…å®¹ä¸è¶…è¿‡é™åˆ¶ï¼Œç›´æ¥åˆ›å»º
    if total_size <= chunk_size:
        result = await create_wiki_page(
            config=config,
            title=title,
            content=content_html,
            space_key=space_key,
            format="html",  # å·²ç»è½¬æ¢è¿‡äº†
            parent_page_id=parent_page_id
        )
        result["chunked"] = False
        result["total_size"] = total_size
        result["chunks"] = 1
        return result

    # 4. å†…å®¹è¿‡é•¿ï¼Œéœ€è¦åˆ†æ‰¹å¤„ç†
    print(f"ğŸ“¦ å†…å®¹å¤§å° {total_size} å­—èŠ‚è¶…è¿‡é™åˆ¶ {chunk_size} å­—èŠ‚ï¼Œå°†åˆ†æ‰¹åˆ›å»º...")

    # 5. å°†å†…å®¹åˆ†å‰²æˆå¤šä¸ªæ‰¹æ¬¡ï¼ˆæ™ºèƒ½åˆ‡åˆ†ï¼Œä¿è¯ HTML å®Œæ•´æ€§ï¼‰
    chunks = []
    offset = 0
    while offset < total_size:
        chunk_end = min(offset + chunk_size, total_size)
        chunk_bytes = content_bytes[offset:chunk_end]

        # é¦–å…ˆå°è¯• UTF-8 è§£ç 
        chunk_text = None
        try:
            chunk_text = chunk_bytes.decode('utf-8')
        except UnicodeDecodeError:
            # å›é€€åˆ°å‰ä¸€ä¸ªå®Œæ•´å­—ç¬¦
            while chunk_end > offset:
                chunk_end -= 1
                chunk_bytes = content_bytes[offset:chunk_end]
                try:
                    chunk_text = chunk_bytes.decode('utf-8')
                    break
                except UnicodeDecodeError:
                    continue

        if chunk_text is None:
            raise RuntimeError("æ— æ³•æ­£ç¡®åˆ†å‰² UTF-8 å†…å®¹")

        # å¦‚æœä¸æ˜¯æœ€åä¸€å—ï¼Œå°è¯•åœ¨ HTML æ ‡ç­¾è¾¹ç•Œå¤„åˆ‡åˆ†ï¼ˆä¼˜åŒ–åˆ‡åˆ†ç‚¹ï¼‰
        if chunk_end < total_size:
            # å°è¯•æ‰¾åˆ°æœ€è¿‘çš„é—­åˆæ ‡ç­¾ä½œä¸ºåˆ‡åˆ†ç‚¹ï¼ˆ</p>, </li>, </td> ç­‰ï¼‰
            # æŒ‰ä¼˜å…ˆçº§æŸ¥æ‰¾ï¼š</p> > </li> > </td> > </div>
            for tag in ['</p>', '</li>', '</td>', '</div>']:
                last_tag_pos = chunk_text.rfind(tag)
                if last_tag_pos > 0 and last_tag_pos > len(chunk_text) // 2:  # è‡³å°‘åœ¨ä¸­ç‚¹ä¹‹å
                    # ä»æ ‡ç­¾åé¢å¼€å§‹ä¸‹ä¸€å—
                    chunk_text = chunk_text[:last_tag_pos + len(tag)]
                    chunk_end = offset + len(chunk_text.encode('utf-8'))
                    break

        chunks.append(chunk_text)
        offset = chunk_end

    print(f"ğŸ“Š å†…å®¹å·²åˆ†ä¸º {len(chunks)} æ‰¹")

    # 6. åˆ›å»ºé¡µé¢ï¼ˆä½¿ç”¨ç¬¬ä¸€æ‰¹å†…å®¹ï¼‰
    print(f"ğŸ“ åˆ›å»ºé¡µé¢ï¼ˆç¬¬ 1/{len(chunks)} æ‰¹ï¼Œ{len(chunks[0].encode('utf-8'))} å­—èŠ‚ï¼‰...")
    result = await create_wiki_page(
        config=config,
        title=title,
        content=chunks[0],
        space_key=space_key,
        format="html",
        parent_page_id=parent_page_id
    )

    page_id = result["id"]
    print(f"âœ… é¡µé¢å·²åˆ›å»ºï¼ŒID: {page_id}")

    # 7. è¿½åŠ å‰©ä½™å†…å®¹
    for i, chunk in enumerate(chunks[1:], start=2):
        print(f"ğŸ“ è¿½åŠ å†…å®¹ï¼ˆç¬¬ {i}/{len(chunks)} æ‰¹ï¼Œ{len(chunk.encode('utf-8'))} å­—èŠ‚ï¼‰...")
        await update_wiki_page_content(
            config=config,
            page_id=page_id,
            content=chunk,
            format="html",
            append=True
        )
        print(f"âœ… ç¬¬ {i} æ‰¹å·²è¿½åŠ ")

    # 8. è¿”å›æœ€ç»ˆç»“æœ
    print(f"ğŸ‰ æ‰€æœ‰å†…å®¹å·²æˆåŠŸæ·»åŠ åˆ°é¡µé¢")
    result["chunked"] = True
    result["total_size"] = total_size
    result["chunks"] = len(chunks)
    result["message"] = f"é¡µé¢å·²æˆåŠŸåˆ›å»ºï¼ˆåˆ† {len(chunks)} æ‰¹æ·»åŠ å†…å®¹ï¼Œæ€»å¤§å° {total_size} å­—èŠ‚ï¼‰ï¼ŒID: {page_id}"

    return result


# ============================================================================
# CLI æ¥å£
# ============================================================================

async def cmd_get(args):
    """è·å–é¡µé¢å†…å®¹å‘½ä»¤"""
    config = WikiConfig()

    try:
        result = await get_wiki_page_content(
            config,
            page_id=args.page_id,
            page_url=args.url,
            format=args.format
        )

        if args.output:
            with open(args.output, 'w', encoding='utf-8') as f:
                f.write(result["content"])
            print(f"âœ… å†…å®¹å·²ä¿å­˜åˆ°: {args.output}")

        if args.json:
            print(json.dumps(result, ensure_ascii=False, indent=2))
        else:
            print(f"ğŸ“„ æ ‡é¢˜: {result['title']}")
            print(f"ğŸ”— URL: {result['url']}")
            print(f"ğŸ“ ç©ºé—´: {result['space']}")
            print(f"ğŸ“Œ ç‰ˆæœ¬: {result['version']}")
            print(f"ğŸ‘¤ æœ€åæ›´æ–°: {result['last_updated_by']} ({result['last_updated']})")
            if result['labels']:
                print(f"ğŸ·ï¸  æ ‡ç­¾: {', '.join(result['labels'])}")
            if result['attachments']:
                print(f"ğŸ“ é™„ä»¶æ•°: {len(result['attachments'])}")
            if not args.output:
                print(f"\n--- å†…å®¹ ---\n{result['content']}")

    except Exception as e:
        print(f"âŒ é”™è¯¯: {e}", file=sys.stderr)
        sys.exit(1)


async def cmd_update(args):
    """æ›´æ–°é¡µé¢å†…å®¹å‘½ä»¤"""
    config = WikiConfig()

    try:
        # å¤„ç† page_id
        page_id = args.page_id
        if args.url and not page_id:
            page_id = extract_page_id(args.url)

        if not page_id:
            raise ValueError("å¿…é¡»æä¾› --page-id æˆ– --url")

        # è¯»å–å†…å®¹
        content = None
        if args.content:
            content = args.content
        elif args.file:
            with open(args.file, 'r', encoding='utf-8') as f:
                content = f.read()

        # æ‰§è¡Œæ›´æ–°
        result = await update_wiki_page_content(
            config,
            page_id=page_id,
            content=content,
            title=args.title,
            format=args.format,
            append=args.append
        )

        print(f"âœ… {result['message']}")
        print(f"ğŸ“„ æ ‡é¢˜: {result['title']}")
        print(f"ğŸ”— URL: {result['url']}")
        print(f"ğŸ“Œ ç‰ˆæœ¬: {result['version']}")
        print(f"ğŸ‘¤ æ›´æ–°è€…: {result['last_updated_by']}")

    except Exception as e:
        print(f"âŒ é”™è¯¯: {e}", file=sys.stderr)
        sys.exit(1)


async def cmd_create(args):
    """åˆ›å»ºé¡µé¢å‘½ä»¤"""
    config = WikiConfig()

    try:
        # è¯»å–å†…å®¹
        content = None
        if args.content:
            content = args.content
        elif args.file:
            with open(args.file, 'r', encoding='utf-8') as f:
                content = f.read()
        else:
            raise ValueError("å¿…é¡»æä¾› --content æˆ– --file")

        # è·å–ç©ºé—´ï¼ˆä¼˜å…ˆä½¿ç”¨å‘½ä»¤è¡Œå‚æ•°ï¼Œå…¶æ¬¡ä½¿ç”¨é»˜è®¤å€¼ï¼‰
        space_key = args.space if args.space else config.default_space
        if not space_key:
            raise ValueError("å¿…é¡»æä¾› --space æˆ–è®¾ç½® WIKI_DEFAULT_SPACE ç¯å¢ƒå˜é‡")

        # è·å–çˆ¶é¡µé¢ï¼ˆä¼˜å…ˆä½¿ç”¨å‘½ä»¤è¡Œå‚æ•°ï¼Œå…¶æ¬¡ä½¿ç”¨é»˜è®¤å€¼ï¼‰
        parent_page_id = args.parent if args.parent else config.default_parent_page_id
        parent_page_id = parent_page_id if parent_page_id else None

        # æ‰§è¡Œåˆ›å»ºï¼ˆå¦‚æœæŒ‡å®šäº† chunk_size åˆ™ä½¿ç”¨åˆ†æ‰¹åˆ›å»ºï¼‰
        if args.chunk_size:
            result = await create_wiki_page_with_chunks(
                config,
                title=args.title,
                content=content,
                space_key=space_key,
                format=args.format,
                parent_page_id=parent_page_id,
                chunk_size=args.chunk_size
            )
        else:
            result = await create_wiki_page(
                config,
                title=args.title,
                content=content,
                space_key=space_key,
                format=args.format,
                parent_page_id=parent_page_id
            )

        print(f"âœ… {result['message']}")
        print(f"ğŸ“„ æ ‡é¢˜: {result['title']}")
        print(f"ğŸ”— URL: {result['url']}")
        print(f"ğŸ“ ç©ºé—´: {result['space']}")
        print(f"ğŸ“Œ ç‰ˆæœ¬: {result['version']}")
        print(f"ğŸ‘¤ åˆ›å»ºè€…: {result['created_by']}")

        # æ˜¾ç¤ºåˆ†æ‰¹ä¿¡æ¯
        if result.get('chunked'):
            print(f"ğŸ“¦ åˆ†æ‰¹åˆ›å»º: {result['chunks']} æ‰¹ï¼Œæ€»å¤§å°: {result['total_size']} å­—èŠ‚")

    except Exception as e:
        print(f"âŒ é”™è¯¯: {e}", file=sys.stderr)
        sys.exit(1)


async def cmd_extract_id(args):
    """æå–é¡µé¢ ID å‘½ä»¤"""
    try:
        page_id = extract_page_id(args.url)
        print(page_id)
    except Exception as e:
        print(f"âŒ é”™è¯¯: {e}", file=sys.stderr)
        sys.exit(1)


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(
        description="Confluence Wiki Manager - Wiki é¡µé¢ç®¡ç†å·¥å…·",
        formatter_class=argparse.RawDescriptionHelpFormatter
    )

    subparsers = parser.add_subparsers(dest='command', help='å¯ç”¨å‘½ä»¤')

    # get å‘½ä»¤
    get_parser = subparsers.add_parser('get', help='è·å–é¡µé¢å†…å®¹')
    get_parser.add_argument('--page-id', help='é¡µé¢ ID')
    get_parser.add_argument('--url', help='é¡µé¢ URL')
    get_parser.add_argument('--format', choices=['markdown', 'storage', 'view'],
                           default='markdown', help='è¾“å‡ºæ ¼å¼ï¼ˆé»˜è®¤: markdownï¼‰')
    get_parser.add_argument('--output', '-o', help='ä¿å­˜å†…å®¹åˆ°æ–‡ä»¶')
    get_parser.add_argument('--json', action='store_true', help='è¾“å‡º JSON æ ¼å¼')

    # update å‘½ä»¤
    update_parser = subparsers.add_parser('update', help='æ›´æ–°é¡µé¢å†…å®¹')
    update_parser.add_argument('--page-id', help='é¡µé¢ ID')
    update_parser.add_argument('--url', help='é¡µé¢ URL')
    update_parser.add_argument('--content', '-c', help='æ–°å†…å®¹ï¼ˆç›´æ¥æä¾›æ–‡æœ¬ï¼‰')
    update_parser.add_argument('--file', '-f', help='æ–°å†…å®¹ï¼ˆä»æ–‡ä»¶è¯»å–ï¼‰')
    update_parser.add_argument('--title', '-t', help='æ–°æ ‡é¢˜')
    update_parser.add_argument('--format', choices=['markdown', 'html'],
                              default='markdown', help='å†…å®¹æ ¼å¼ï¼ˆé»˜è®¤: markdownï¼‰')
    update_parser.add_argument('--append', '-a', action='store_true',
                              help='è¿½åŠ å†…å®¹ï¼ˆä¸è¦†ç›–åŸæœ‰å†…å®¹ï¼‰')

    # create å‘½ä»¤
    create_parser = subparsers.add_parser('create', help='åˆ›å»ºæ–°é¡µé¢')
    create_parser.add_argument('--title', '-t', required=True, help='é¡µé¢æ ‡é¢˜ï¼ˆå¿…éœ€ï¼‰')
    create_parser.add_argument('--content', '-c', help='é¡µé¢å†…å®¹ï¼ˆç›´æ¥æä¾›æ–‡æœ¬ï¼‰')
    create_parser.add_argument('--file', '-f', help='é¡µé¢å†…å®¹ï¼ˆä»æ–‡ä»¶è¯»å–ï¼‰')
    create_parser.add_argument('--space', '-s', help='ç©ºé—´ keyï¼Œä¾‹å¦‚: ~ht, SPACEï¼ˆå¦‚æœæœªæŒ‡å®šåˆ™ä½¿ç”¨ WIKI_DEFAULT_SPACEï¼‰')
    create_parser.add_argument('--parent', '-p', help='çˆ¶é¡µé¢ IDï¼ˆå¦‚æœæœªæŒ‡å®šåˆ™ä½¿ç”¨ WIKI_DEFAULT_PARENT_PAGEï¼‰')
    create_parser.add_argument('--format', choices=['html', 'markdown'],
                              default='html', help='å†…å®¹æ ¼å¼ï¼ˆé»˜è®¤: htmlï¼‰')
    create_parser.add_argument('--chunk-size', type=int, help='å½“å†…å®¹è¶…è¿‡æ­¤å­—èŠ‚æ•°æ—¶åˆ†æ‰¹åˆ›å»ºï¼ˆå¦‚: 1048576 è¡¨ç¤º 1MBï¼‰')


    # extract-id å‘½ä»¤
    extract_parser = subparsers.add_parser('extract-id', help='ä» URL æå–é¡µé¢ ID')
    extract_parser.add_argument('url', help='é¡µé¢ URL')

    args = parser.parse_args()

    if not args.command:
        parser.print_help()
        sys.exit(1)

    # æ‰§è¡Œå‘½ä»¤
    if args.command == 'get':
        asyncio.run(cmd_get(args))
    elif args.command == 'update':
        asyncio.run(cmd_update(args))
    elif args.command == 'create':
        asyncio.run(cmd_create(args))
    elif args.command == 'extract-id':
        asyncio.run(cmd_extract_id(args))


if __name__ == '__main__':
    main()
